# Neural-Network In Python
This program allows you to design, train, test and run basic neural networks using a selection of different layers which are :  
Convolutional, Max and Average Pooling, Drop out, Fully Connected, Batch Normalization, and Flatten.  

In conjunction with the activations:  
ReLu, LeakyReLu and Softmax.  

The program also facilitates basic random image augmentation including:  
Scaling, Rotation, Noise, and Translation.  

All weights are trained with the adam optimizer algorithm.  

All layers and activations are written in python, from scratch (of course, not counting the use of numpy for various matrix calculations).

This project was undertaken to learn the fundamental concepts behind neural networks and deep learning, as well as to gain experience with numpy and matrixes. In future projects I will of course use tensorflow or pytorch simply for speed of both development and training, however i hope to come back to this project to try to develop new techniques or teach myself how other layers and optimization algorithms work.
